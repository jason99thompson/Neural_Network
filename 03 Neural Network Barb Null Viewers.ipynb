{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "\n",
    "# library for plotting arrays\n",
    "import matplotlib.pyplot\n",
    "# ensure the plots are inside the notebook, not an external window\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nerual network class definition\n",
    "class nerualNetwork:\n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih, who\n",
    "        # weights inside the arrays are w_i_j,where linnk is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc\n",
    "        # simple version\n",
    "        # self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5)\n",
    "        # self.who= (numpy.random.rand(self.onodes, self.hnodes) - 0.5)\n",
    "        # Based upon distribution p133\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))    \n",
    "        \n",
    "        #learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T ##WHAT IS THIS???\n",
    "        targets = numpy.array(targets_list, ndmin=2).T ##WHAT IS THIS???\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calulate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs) \n",
    "        \n",
    "        # error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        \n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors)\n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T ##WHAT IS THIS???\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calulate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at training data\n",
    "#with open (\"EM_SAVE_TRAIN_DT11.csv\", 'r') as f:\n",
    "#    training_data_list = f.readlines()\n",
    "#f.closed\n",
    "\n",
    "\n",
    "f = open(\"EM_SAVE_TRAIN_DT11.csv\", 'r')\n",
    "training_data_list = f.readlines() ## for large files better to read a line at a time\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target_date', 'household_number', 'day_00_', 'day_01_', 'day_02_', 'day_03_', 'day_07_', 'first28_ViewTV', '_dataobs_', '_WARN_\\n']\n"
     ]
    }
   ],
   "source": [
    "# Read header row\n",
    "record_count = 0\n",
    "\n",
    "for record in training_data_list:\n",
    "    if record_count == 0:\n",
    "        all_values = record.split(',')\n",
    "        print(all_values)\n",
    "        pass\n",
    "    record_count += 1\n",
    "    pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of input, hidden, output nodes\n",
    "input_nodes = 5\n",
    "hidden_nodes = 40\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.3\n",
    "\n",
    "# create instance of neural network\n",
    "n = nerualNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "# load the mnist training data csv file into a list\n",
    "f = open(\"EM_SAVE_TRAIN_DT11.csv\", 'r')\n",
    "training_data_list = f.readlines() ## for large files better to read a line at a time\n",
    "f.close()\n",
    "\n",
    "# train the nerual network\n",
    "\n",
    "record_count = 0\n",
    "\n",
    "# Read and format the training file\n",
    "# target_date, household number, day_00_, day_01_, day_02_, day_03, day_07_, first28_ViewTV, dataobs_, _WARN_ \n",
    "for record in training_data_list:\n",
    "    # first record is header, do not process\n",
    "    if record_count >0: #and record_count <=10:\n",
    "        # split the record by the \",\"commas\n",
    "        all_values = record.split(',')\n",
    "        #print(record_count, \": Original All: \", all_values)\n",
    "        # Give values to No_Data, No_View, View_TV\n",
    "        dic = {\"No_Data\":0.0 / 2.0 * 0.99 + 0.01, \"No_View\":1.0 / 2.0 * 0.99 + 0.01, \"View_TV\": 2.0 / 2.0 * 0.99 + 0.01}    \n",
    "        all_values = [dic[n] if n in dic else n for n in all_values]\n",
    "        # Rescale first28_ViewTV\n",
    "        all_values[7] = float(all_values[7]) / 28.0 * 0.99 + 0.01 \n",
    "\n",
    "        # inputs\n",
    "        inputs = numpy.asfarray(all_values[3:8])\n",
    "        #print(record_count, \": Processed All: \", all_values)\n",
    "        #print(record_count, \": Inputs: \", inputs)\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        # all_values[2] is the target label for this record\n",
    "        targets[int(all_values[2] * 2)] = 0.99\n",
    "        #print(record_count, \": Target: \", targets)\n",
    "        #Boost the number of \"No_view\" as only 5% of data\n",
    "        #counter = 0\n",
    "        #while all_values[2] == 1.0 and counter < 1:\n",
    "        n.train(inputs, targets)\n",
    "        #counter += 1\n",
    "        pass\n",
    "    record_count += 1\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the neural network\n",
    "\n",
    "# scorecard for how well the network performs, initially empty \n",
    "#scorecard_0 = []\n",
    "#scorecard_1 = []\n",
    "#scorecard_2 = []\n",
    "scorecard = []\n",
    "\n",
    "# load the test data csv file into a list\n",
    "test_data_file = open(\"EM_SAVE_TEST.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "\n",
    "\n",
    "record_count = 0\n",
    "\n",
    "# Read and format the test file\n",
    "# target_date, household number, day_00_, day_01_, day_02_, day_03, day_07_, first28_ViewTV, dataobs_, _WARN_ \n",
    "for record in test_data_list:\n",
    "    # first record is header, do not process\n",
    "    #print(record_count)\n",
    "    if record_count > 0:\n",
    "        # split the record by the \",\"commas\n",
    "        all_values = record.split(',')\n",
    "        #print(all_values)\n",
    "        # Give values to No_Data, No_View, View_TV\n",
    "        dic = {\"No_Data\":0.0 / 2.0 * 0.99 + 0.01, \"No_View\":1.0 / 2.0 * 0.99 + 0.01, \"View_TV\": 2.0 / 2.0 * 0.99 + 0.01}    \n",
    "        all_values = [dic[n] if n in dic else n for n in all_values]\n",
    "        # Rescale first28_ViewTV\n",
    "        all_values[7] = float(all_values[7]) / 28.0 * 0.99 + 0.01\n",
    "        #print(all_values)\n",
    "        # inputs\n",
    "        inputs = numpy.asfarray(all_values[3:8])\n",
    "        #print(inputs)\n",
    "        # outputs\n",
    "        outputs = n.query(inputs)\n",
    "        # correct answer is in index 2\n",
    "        correct_label = int(all_values[2] * 2)\n",
    "        #print(correct_label)\n",
    "        # the index of the highest value corresponds to the label\n",
    "        label = numpy.argmax(outputs)\n",
    "        #print(label)\n",
    "        if (label == correct_label):\n",
    "            # network's answer matches correct answer, add 1 to scorecard\n",
    "            scorecard.append([1, float(outputs[0]), float(outputs[1]), float(outputs[2]), float(outputs[label]) / outputs.sum(), correct_label, label])\n",
    "        else:\n",
    "            # networks answer doesn't match correct answer, add 0 to scorecard\n",
    "            scorecard.append([0,float(outputs[0]), float(outputs[1]), float(outputs[2]), float(outputs[label] / outputs.sum()), correct_label, label])\n",
    "            pass        \n",
    "                \n",
    "        pass\n",
    "    record_count += 1\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance =  0.939510964613\n"
     ]
    }
   ],
   "source": [
    "# calculate the performance score, the function of correct answers\n",
    "scorecard_array = numpy.asarray(scorecard)\n",
    "print(\"Performance = \", sum(scorecard_array[:, 0]) / len(scorecard_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Data:  0.976452816776\n",
      "No View:  0.632062146893\n",
      "View TV:  0.945500690647\n"
     ]
    }
   ],
   "source": [
    "score_summary = numpy.zeros((3, 3))\n",
    "\n",
    "for x in range(len(scorecard_array)):\n",
    "    target_index = int(scorecard_array[x, 5])\n",
    "    model_index = int(scorecard_array[x, 6])\n",
    "    score_summary[target_index, model_index] += 1\n",
    "    #print(target_index, model_index, score_summary[target_index, model_index])\n",
    "\n",
    "print(\"No Data: \", score_summary[0,0] / sum(score_summary[0, :]))\n",
    "print(\"No View: \", score_summary[1,1] / sum(score_summary[1, :]))\n",
    "print(\"View TV: \", score_summary[2,2] / sum(score_summary[2, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LR = 0.3\n",
    "No Data:  0.976648302826\n",
    "No View:  0.389712806026\n",
    "View TV:  0.96863454131\n",
    "\n",
    "# LR = 0.1    \n",
    "No Data:  0.975866358628\n",
    "No View:  0.173375706215\n",
    "View TV:  0.979347984772  \n",
    "    \n",
    "# LR = 0.5\n",
    "No Data:  0.976346188022\n",
    "No View:  0.129472693032\n",
    "View TV:  0.982537311757\n",
    "    \n",
    "# LR = 0.4\n",
    "No Data:  0.97620401635\n",
    "No View:  0.31450094162\n",
    "View TV:  0.974002493065\n",
    "    \n",
    "# LR = 0.2    \n",
    "No Data:  0.976097387596\n",
    "No View:  0.22233992467\n",
    "View TV:  0.976338338181\n",
    "    \n",
    "# Nodes = 20\n",
    "No Data:  0.975262129021\n",
    "No View:  0.47634180791\n",
    "View TV:  0.957179916224 \n",
    "    \n",
    "# Nodes = 30\n",
    "No Data:  0.976275102186\n",
    "No View:  0.631002824859\n",
    "View TV:  0.945040259638\n",
    "\n",
    "# Nodes = 40    \n",
    "No Data:  0.976186244891\n",
    "No View:  0.630532015066\n",
    "View TV:  0.947039204016 \n",
    "    \n",
    "# No_View x 7\n",
    "No Data:  0.976186244891\n",
    "No View:  0.630532015066\n",
    "View TV:  0.947039204016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.49300000e+04   1.71000000e+02   1.16900000e+03]\n",
      " [  7.30000000e+01   5.35700000e+03   3.06600000e+03]\n",
      " [  1.25100000e+03   3.46500000e+03   8.43310000e+04]]\n",
      "0.365833837192\n",
      "0.0552359033372\n",
      "0.578930259471\n"
     ]
    }
   ],
   "source": [
    "print(score_summary)\n",
    "score_summary.sum()\n",
    "\n",
    "\n",
    "print(sum(score_summary[0, :]) / score_summary.sum())\n",
    "print(sum(score_summary[1, :]) / score_summary.sum())\n",
    "print(sum(score_summary[2, :]) / score_summary.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
